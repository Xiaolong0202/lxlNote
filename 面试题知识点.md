## Redis

![image-20231115214504285](mdPic/面试题知识点/image-20231115214504285.png)

延时队列使用zset

消息队列使用list

![image-20231115214554996](mdPic/面试题知识点/image-20231115214554996.png)

**缓存穿透：** 每次都会查询一个不存在的数据，导致每次请求都去查询数据库，

解决方案：

1.**缓存空数据**2.**使用布隆过滤器，**在使用缓冲之前先查询布隆数据库，拦截布隆过滤器（
可以使用位图bitmap实现）,将key进行多次hash，根据多次hash将对应的数据bit变为1，查询数据的时候就可以使用hash函数判断是否存在（可能存在误判，即某个不存在的也可被误判为存在）

**缓存击穿** 某一个key刚好过期大量的请求进来将DB压垮

两种解决方案：互斥锁（数据的一致性），与逻辑过期（可用性）

![image-20231115215804896](mdPic/面试题知识点/image-20231115215804896.png)

**缓存雪崩** 指集体失效，比如许多热点key在同一时间过期（可以给每个key设置随机的过期时间），**redis宕机**，可以搭建redis的集群来预防,哨兵集群模式

添加多级缓存

添加限流

**双写一致：**保证数据库与redis缓存的一致性

措施:

延时双删(不能保证数据强一致性),延时是想让主节点数据库数据扩散至从结点:

![image-20231116203045509](mdPic/面试题知识点/image-20231116203045509.png)

**保证强一致性的方法，**对一个数据的读或写都添加分布式锁,但是性能会下降

对于读多写少的场景，我们可以使用共享锁与排他锁结合使用，

![image-20231116203848247](mdPic/面试题知识点/image-20231116203848247.png)

**保证最终一致性**，可以使用消息队列,先修改数据库，使用异步通知的方式更新缓存保证最终一致性

![image-20231116204153597](mdPic/面试题知识点/image-20231116204153597.png)

**redis的持久化**

**RDB**:就是将数据以文件的方式存储至磁盘当中,数据快照

可以使用save(主进程)与bgsave(子进程)命令执行RDB

可以使用redisconfig来配置rdb触发机制

![image-20231116211012782](mdPic/面试题知识点/image-20231116211012782.png)

![image-20231116211431787](mdPic/面试题知识点/image-20231116211431787.png)

**AOF:**记录命令的日志

在配置文件中配置appendonly打开

![image-20231116211538098](mdPic/面试题知识点/image-20231116211538098.png)

几种频率:

![image-20231116211608581](mdPic/面试题知识点/image-20231116211608581.png)

为了防止命令的冗余，可以使用bgrewriteaof命令，执行aof文件的重写减少aof文件大小

![image-20231116211757952](mdPic/面试题知识点/image-20231116211757952.png)

百分比与体积两种重写的阈值

AOF与RDB对比：

![image-20231116211854147](mdPic/面试题知识点/image-20231116211854147.png)

**redis key的过期后的删除策略:**

1.惰性删除，在过期之后还在内存当中不去管他，当再次使用到他的时候在对他进行检测，如果过期了的话就删除,(对CPU有好，但是对内存不友好比较浪费)

2.定期删除，每隔一定的时间，就会遍历定量的key,删除里面过期的key,

定期删除的时候有两种模式：

![image-20231116212617433](mdPic/面试题知识点/image-20231116212617433.png)

定期删除执行时间少的原因是尽量的不占用CPU资源

![image-20231116212737404](mdPic/面试题知识点/image-20231116212737404.png)



**redis的淘汰策略:**

当redis内存不够用的时候需要对redis中的key进行淘汰,八种淘汰策略

![image-20231116214008631](mdPic/面试题知识点/image-20231116214008631.png)

redis分布式锁，使用set nx  ex命令实现,可以使用看门狗机制使用守护线程来进行锁的续期,每隔releaseTime/3的时间续期，redssion实现的分布式锁是可重入锁

为了保证主从数据的一致性，也就是怕主节点突然宕机，redission还实现了红锁,在多个节点上创建锁(实现起来特别复杂)：

![image-20231116220250264](mdPic/面试题知识点/image-20231116220250264.png)

红锁实现复杂成本高，如果非要保证强一致性，就要使用zookeeper（CP），redis是(AP)

**Redis集群的方案：**

**1.主从复制**

主节点一般实现写操作，从节点一般实现读操作，主节点写入数据的时候需要同步给从节点

**同步的情况**

1.主从全量同步

![image-20231116221102627](mdPic/面试题知识点/image-20231116221102627.png)

![image-20231116221418159](mdPic/面试题知识点/image-20231116221418159.png)

**哨兵机制**

监控主从结点的故障，

故障恢复，如果主节点宕机了，会推选从节点充当新的从节点，

通知，推举新的主节点，故障转移的时候会通知Redis客户端

![image-20231116222835561](mdPic/面试题知识点/image-20231116222835561.png)

![image-20231116223003718](mdPic/面试题知识点/image-20231116223003718.png)

**脑裂问题：**

因为网络故障问题，产生了两个主节点，客户端还在往原来的主节点中写入数据，等到网络恢复的时候，redis集群会将之前的主节点当做salve从节点,数据清空，导致数据丢失

![image-20231116223627717](mdPic/面试题知识点/image-20231116223627717.png)

解决方案：通过配置来使得被分离的老的主节点拒绝客户端的请求

![image-20231116223430715](mdPic/面试题知识点/image-20231116223430715.png)

**分片集群** 可以支持海量数据，每个结点master都存储不同的数据：

![image-20231116230638676](mdPic/面试题知识点/image-20231116230638676.png)

可以使用哈希槽的方法来实现决定一个key对应的位置应该放在哪里

**Redis是单线程的为什么这么快**

- c语言编写的,他是内存数据库，他可以直接操纵内存，当然快
- 他是单线程的，不需要进行上下文的切换,切换可竞争条件
- 它的网络IO是非阻塞IO，使用了IO多路复用模型

Redis是纯内存模型，所以说他的性能瓶颈其实是网络IO，I/O多路复用就是为了实现高效的网络请求

**传统io模型降低效率的两个点：**

-  用户空间与内核空间频繁的数据拷贝就降低了效率，
- 还有是用户缓冲区一直等待内核缓冲区的数据也会降低效率

![image-20231117145236224](mdPic/面试题知识点/image-20231117145236224.png)

**阻塞IO**

![image-20231117145420090](mdPic/面试题知识点/image-20231117145420090.png)

**非阻塞IO**

![image-20231117145607981](mdPic/面试题知识点/image-20231117145607981.png)

**IO多路复用**

![image-20231117145824175](mdPic/面试题知识点/image-20231117145824175.png)

**Linux实现IO多路复用的三种方式**

- select、poll 这两种方式只会通知用户进程有socket的资源已经准备好了，但是不会告诉用户进程是哪一个socket资源准备好了，需要用户进程去进行轮询，
- epoll ,在通知用户进程socket进程准备好的时候，会顺便告诉用户进程是哪一个用户进程准备好了,就可以直接将socket放入用户空间

**Redis的网络模型,6.0之后引入了多线程**

多线程主要是对IO部分添加了多线程的处理

IO复用+事件派发

![image-20231117152808545](mdPic/面试题知识点/image-20231117152808545.png)

# MYSQL

mysql定位慢查询

可以使用sql的日志

![image-20231117161509430](mdPic/面试题知识点/image-20231117161509430.png)

![image-20231117162547818](mdPic/面试题知识点/image-20231117162547818.png)

使用explain对sql执行计划进行分析

![image-20231117163057736](mdPic/面试题知识点/image-20231117163057736.png) 

![image-20231117163357429](mdPic/面试题知识点/image-20231117163357429.png)

**索引的概念：**

​	索引(index) 是帮助MySQL高效获取数据的数据结构(有序)。在数据之外，数据库系统还维护着满足特定查找算法的数据结构(B+树)，这些数据结构以某种方式引用(指向)数据，这样就可以在这些数据结构， 上实现高级查找算法，这种数据结构就是索引。对查找的字段进行排序

**索引的底层数据结构**

使用B+树

优点

- 使用的是矮胖树，阶层更低，路径更短
- 磁盘读写的代价更低，非叶子结点只存储指针，只有叶子结点是存储的数据
- B+树的叶子结是一个双向链表，更利于扫库和查询范围

**索引的分类**

- 聚集索引，  一般是针对于主键的索引。聚集索引一张表当中一般只存在一个，该索引结构中存放数据的部分存储着整行的所有字段的数据 ,数据与索引放到一块，B+树的叶子节点保存了整行数据，有且只有一个

  一般会将给一个表的主键创建索引，如果一个表中没有主键，就会给这个表当中的唯一键创建聚集索引，如果也没有唯一键，数据库会给表中的没有每一行创建一个rowid，给这个rowid创建聚集索引

- 二级索引、非聚集索引、普通索引，该索引，一张表当中可以创建多个，但是该种索引存放数据的部分，存放的是该数据的主键，数据与索引分开存储，B+树的叶子节点保存对应的主键，可以有多个

**回表**

> 通过二级索引找到对应的主键值，到聚集索引中查找整行数据，这个过程就是回表

**覆盖索引 ** 覆盖索引是指查询使用了索引，返回的列，必须在索引中全部能够找到，不用回表的就是覆盖索引。

使用id查询，直接走聚集索引查询，一次索引扫描，直接返回数据，性能高
如果返回的列中没有创建索引，有可能会触发回表查询，尽量避免使用select*

使用覆盖索引解决超大问题：

覆盖索引子查询

![image-20231117174803721](mdPic/面试题知识点/image-20231117174803721.png)

**索引创建的原则：**

+ 对于数据量大的表创建索引，增加用户体验，如数据量大于十万
+ 对于经常作为where 、order by、group by 操作的字段，创建索引
+ 尽量选择区分度较高的字段创建索引，这样效率更高
+ 对于字符创类型的字段如果长度较长可以针对字段的特点创建前缀索引
+ 要尽量使用联合索引，使用联合索引的时候很多时候可以覆盖索引，可以节省空间，避免回表，提高查询的效率
+ 控制索引的数量，索引不是越多越好

**索引失效的条件**

- 违反了最左前缀法则，比如一个联合索引，查询的时候的条件的列必需从左开始，不能跳过索引
-  范围查询的右边的列是不能使用使用索引的
- 对索引的列进行运算
- 条件查询的时候，对字符串不加单引号会使得索引失效，因为会有类型转换
- 模糊查询的时候不能使用头部模糊匹配，不然会失效

**SQL语句的优化**

> SELECT语句务必指明字段名称(避免直接使用select * )
> SQL语句要避免造成索弓|失效的写法
> 尽量用union all代替union union会多- 次过滤， 效率低
> 避免在where子句中对字段进行表达式操作
> Join优化能用innerjoin就不用left join right join,如必须使用-定要以小表为驱动,
> 内连接会对两个表进行优化，优先把小表放到外边,把大表放到里边。left join或right join,不会重新调整顺序

![image-20231117220913459](mdPic/面试题知识点/image-20231117220913459.png)

**数据库事务**

事务是一组操作的集合，它是一个不可分割的工作单位，事务会把所有的操作作为一个整体一起向系统提交或撤销操作请求，即这些操作要么同时成功，要么同时失败。

事务特性ACID

- 原子性  事务是操作的最小单元要么成功要么失败
- 一致性  事务完成时，所有的数据应该保持一致
- 隔离性  事务执行的时候，应该保证不受到外界并发的影响
- 持久性  事务提交或回滚之后。所有的数据都将持久化存储

例子转账，要么成功要么失败a扣除1000b就要增加1000，在转账时，不能受其他的影响，转账之后数据局要存储在磁盘中

**数据库并发的三种问题**

- 脏读  一个事务读到了另外一个事务的还没有提交的数据
- 不可重复读   在同一次事务当中读取同一条数据两次，两次读取到的数据不一样
- 幻读   在同一次事务当中第一次查询数据时候，某一条数据不存在，但是第二次操纵数据库的时候却发现那一条数据又存在了

**数据库的隔离级别**

| 隔离级别                             | 脏读 | 不可重复读 | 幻读 |
| ------------------------------------ | ---- | ---------- | ---- |
| 读未提交（Read Uncommitted）         | 是   | 是         | 是   |
| 读已提交（Read Committed）           | 否   | 是         | 是   |
| 可重复读（Repeatable Read）MYSQL默认 | 否   | 否         | 是   |
| 串行化（Serializable）               | 否   | 否         | 否   |



**缓冲池(buffer pool)**:

> 主内存中的一个区域，里面可以缓存磁盘上经常操作的真实数据，在执行增删改查操作时，先操作缓冲池中的数据(若缓冲池没有数据，则从磁盘加载并缓存》，以一定频率刷新到磁盘，从而减少磁盘10，加快处理速度

**数据页(page)**

> 是lnnoDB 存储引擎磁盘管理的最小单元，每个页的大小默认为 16KB。页中存储的是行数据

**redo log**

redo log是用来实现事务的持久性。

重做日志，记录的是事务提交时数据页的物理修改,是用来实现事务的持久性。
该日志文件由两部分组成:重做日志缓冲(redo log buffer)以及重做日志文件(redo log file) ,前者是在内存中，后者在磁盘中。当事
务提交之后会把所有修改信息都存到该日志文件中，用于在刷新脏页到磁盘,发生错误时,进行数据恢复使用。

![image-20231117231053994](mdPic/面试题知识点/image-20231117231053994.png)

**undo log**

undo log可以实现事务的一致性和原子性

回滚日志，用于记录数据被修改前的信息，作用包含两个:提供回滚和 MVCC(多版本并发控制)。undo log和redo log记录物理日志不一样，它是逻辑日志
可以认为当delete一条记录时，undo log中会记录一条对应的insert记录，反之亦然当update一条记录时，它记录一条对应相反的update记录。当执行rolback时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。
undo log可以实现事务的一致性和原子性

**MySql数据库隔离的控制的实现方式**

1. 锁 排它锁 共享锁
2. mvcc 多版本并发控制

**MVCC 多版本并发控制**





**主从同步的原理**

MySQL主从复制的核心就是二进制日志
二进制日志(BINLOG)记录了所有的 DDL(数据定义语言)语和 DML(数据操纵语言)语句，但不包括数据查询(SELECT、SHOW)语句.

复制分成三步
1.Master主库在事务提交时，会把数据变更记录在二进制日志文件 Binlog 中
2.从库读取主库的二进制日志文件 Binlog，写入到从库中继日志 Relay Log
3.slave重做中继日志中的事件，将改变反映它自己的数据

![image-20231117235247955](mdPic/面试题知识点/image-20231117235247955.png)

**SQL语句的五大类**

- DQL (Data Query Language-数据查询语言) - Select 查询语句不存在提交问题。
- DML (Data Manipulation Language-数据操作语言) - Insert、Update、Delete，实现数据的“增删改。 ”
- DDL (Data Definition Language-数据定义语言) - Create、Alter、Drop，实现表格的“增删改”。
- DTL (Transaction Control Language-事务控制语言) - Commit、Rollback事务提交、回滚语句。
- DCL (Data Control Language-数据控制语言) - Grant、Revoke 权限语句

**分库分表**

数据量达到一定量级就要分库分表 1000W或20G

策略

![image-20231118002345931](mdPic/面试题知识点/image-20231118002345931.png)

垂直分库

![image-20231118002440971](mdPic/面试题知识点/image-20231118002440971.png)

垂直分表

![image-20231118002736590](mdPic/面试题知识点/image-20231118002736590.png)

水平分库,解决海量数据的存储问题，讲一个库中的数据分开

![image-20231118002920174](mdPic/面试题知识点/image-20231118002920174.png)

水平分表：

![image-20231118011223173](mdPic/面试题知识点/image-20231118011223173.png)

水平分库分表产生的问题

![image-20231118011357872](mdPic/面试题知识点/image-20231118011357872.png)

#  spring

**bean线程安全**

spring的中的bean不是线程安全的，因为他默认是单例的，spring中的bean对象可以使用@scope注解来决定是否单例，它默认的值是单例,对于有可修改的成员变量的bean他是有线程安全问题的，需要考虑线程安全问题，需要通过多例或者加锁来解决问题

**spring aop**

spring当中的aop是面向切面编程，是将一些与业务逻辑无关的但是会对多个对象产生影响的逻辑与公共行为，抽取为公共模块，降低耦合

aop的一些使用，可以用来记录日志，记录缓存，spring自身用于实现事务也是用的aop

spring事务使用Transactional守住接来实现，本质是使用aop拦截被注解的service方法，在执行方法之前通过TransactionTemplate编程式开启事务，在方法执行完毕之后依据情况回滚或者提交事务

**Spring事务失效的几种场景**

- 方法中的异常被捕获但是没有被抛出，这样的话你自己本身知道了异常的发生，但是aop代理类并不知道你的异常的发生，这样就不会回滚
- 方法抛出了检查异常，检查异常是出自Exception类的，非检查异常是出自RunTimeException类的，解决方法，添加注解属性：@Transactional(rollbackFor=Exception.class)
- 方法为非public方法的时候注解会失效，因为Spring创建代理类需要目标的方法是public方法
- 使用方法内调用，也就是在同一个类对象当中发生了调用,对应的标注了@Transactinal注解的方法,这样子调用不会调用代理方法，所以事务也不会生效

**bean的生命周期：**

1. **获取bean的定义信息**：通过BeanDefinition获取bean的定义信息。
2. **实例化bean**：调用构造器实例化bean。
3. **依赖注入**：进行bean的依赖注入。
4. **处理Aware接口**：处理Aware接口，包括BeanNameAware、 BeanFactoryAware、 ApplicationContextAware。
5. **后置处理器前置**：进行Bean的后置处理器postProcessor前置。
6. **初始化方法**：执行初始化方法，包括InitializingBean、 init-method。
7. **后置处理器后置**：进行Bean的后置处理器postProcessor后置。
8. **销毁bean**：销毁bean。

![image-20231118154327200](mdPic/面试题知识点/image-20231118154327200.png)

![image-20231118154300598](mdPic/面试题知识点/image-20231118154300598.png)

**代码示例**

User类

```java
package com.lxl.beans;

import org.springframework.beans.BeansException;
import org.springframework.beans.factory.BeanFactory;
import org.springframework.beans.factory.BeanFactoryAware;
import org.springframework.beans.factory.BeanNameAware;
import org.springframework.beans.factory.InitializingBean;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.ApplicationContext;
import org.springframework.context.ApplicationContextAware;
import org.springframework.stereotype.Component;

import javax.annotation.PostConstruct;

/**
 * @Author LiuXiaolong
 * @Description test-autoConfig
 * @DateTime 2023/11/18  16:00
 **/
@Component
public class User implements BeanNameAware, BeanFactoryAware, ApplicationContextAware, InitializingBean {

    private BeanFactory beanFactory;
    private String beanName;
    private ApplicationContext applicationContext;



    @Value("刘孝龙")
    private String userName;

    public String getUserName() {
        return userName;
    }

    public void setUserName(String userName) {
        this.userName = userName;
    }

    public void showSomething(){
        System.out.println("asdasdasdaasasdasda啊实打实大");
    }


    /**
     * 自定义初始化方法
     *
     */
    @PostConstruct
    public void init(){
        System.out.println("PostConstruct 初始化");
    }

    @Override
    public void setBeanFactory(BeanFactory beanFactory) throws BeansException {
        System.out.println("setBeanFactory");
            this.beanFactory = beanFactory;
    }

    @Override
    public void setBeanName(String s) {
        System.out.println("setBeanName");
        this.beanName = s;
    }

    @Override
    public void setApplicationContext(ApplicationContext applicationContext) throws BeansException {
        System.out.println("setApplicationContext");
        this.applicationContext = applicationContext;
    }

    @Override
    public void afterPropertiesSet() throws Exception {
        System.out.println("InitializingBean  afterPropertiesSet");
    }
}

```

MyBeanPostProcessor类

```java
package com.lxl.beans;

import org.springframework.beans.BeansException;
import org.springframework.beans.factory.config.BeanPostProcessor;
import org.springframework.stereotype.Component;

/**
 * @Author LiuXiaolong
 * @Description test-autoConfig
 * @DateTime 2023/11/18  16:25
 **/
@Component
public class MyBeanPostProcessor implements BeanPostProcessor {
    @Override
    public Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException {
        System.out.printf("postProcessBeforeInitialization(Object bean  %s, String beanName  %s)\n", bean.toString(), beanName);
        return BeanPostProcessor.super.postProcessBeforeInitialization(bean, beanName);
    }

    @Override
    public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {
        System.out.printf("postProcessAfterInitialization(Object bean  %s, String beanName  %s)\n", bean.toString(), beanName);
        return BeanPostProcessor.super.postProcessAfterInitialization(bean, beanName);
    }
}

```

Configuration配置类

```java
@Configuration
@ComponentScan("com.lxl.beans")
public class MyConfiguration {

}
```

Main主函数

```java
public class Main {
    public static void main(String[] args) {
        ApplicationContext applicationContext = new AnnotationConfigApplicationContext(MyConfiguration.class);
        User bean = applicationContext.getBean(User.class);
        System.out.println(bean);
    }
}
```

**Spring循环依赖**

> 循环依赖其实就是循环引用,也就是两个或两个以上的bean互相持有对方,最终形成闭环。比如A依赖于B,B依赖于A

spring中的三级缓存

| 保存类型 | 属性名                | 描述                                                         |
| :------- | :-------------------- | :----------------------------------------------------------- |
| 一级缓存 | singletonObjects      | 单例池中，缓存已经解析了完整的bean定义信息，已经实例化并完成初始化的bean对象 |
| 一级缓存 | earlySingletonObjects | 缓存早期的bean对象（在bean属性还没完全注入）                 |
| 三级缓存 | singletonFactories    | 缓存的是ObjectFactory，表示对象实例工厂，用来创建委托个人对象的 |

spring通过三级缓存解决循环依赖问题：

![image-20231118165851806](mdPic/面试题知识点/image-20231118165851806.png)

**SpringMVC的执行流程**

- 视图阶段（JSP,thymleaf）

![image-20231118182533957](mdPic/面试题知识点/image-20231118182533957.png)

- 前后端分离开发

![image-20231118182603065](mdPic/面试题知识点/image-20231118182603065.png)

**springboot自动装配的原理**

![image-20231118201615377](mdPic/面试题知识点/image-20231118201615377.png)

Spring相关注解

| 注解                                            | 说明                                                         |
| :---------------------------------------------- | :----------------------------------------------------------- |
| @Component、@Controller、@Service、@Repository  | 使用这些上面开发我们Bean                                     |
| @Autowired                                      | 使用在字段上用于将指定类型的类装配进入该字段                 |
| @Qualifier                                      | 结合@Autowired一起使用用于将指定名称的类装配进入该字段       |
| @Scope                                          | 标注Bean的作用范围                                           |
| @Configuration                                  | 指定当前类是一个 Spring 配置类，创建容器时会从该类上获取配置信息 |
| @ComponentScan                                  | 用于扫描 Spring 容器初始化时需要扫描的包                     |
| @Bean                                           | 使用在方法上，标识该方法返回值是一个Spring容器中的Bean       |
| @Import                                         | 使用在 @Import导入外部容器Spring加载到IOC容器中              |
| @Aspect、@Before、 @After、 @Around、 @Pointcut | 用于切面编程（AOP）                                          |

springMVC

| 注解            | 说明                                                         |
| :-------------- | :----------------------------------------------------------- |
| @RequestMapping | 用于映射请求路径，可以定义在类上和方法上。用于类上，则表示类中的所有方法都可以处理此路径下的请求。用于方法上，则表示该方法用于处理对应路径下的请求 |
| @RequestBody    | 注解修饰http请求体json数据。将json转换为java对象             |
| @RequestParam   | 指定请求参数的名称和类型                                     |
| @PathVariable   | 从请求路径中获取参数(如/user/{id})，传递给方法的形式参数     |
| @ResponseBody   | 注解修饰controller方法返回值时将值转化为json对象返回客户端   |
| @RequestHeader  | 获取指定的请求头数据                                         |
| @RestController | @Controller + @ResponseBody                                  |

springboot

| 注解                     | 说明                                               |
| :----------------------- | :------------------------------------------------- |
| @SpringBootConfiguration | 注解，实现配置类上的特殊处理                       |
| @EnableAutoConfiguration | 打开自动配置功能，也可以关闭某个自动配置选项的功能 |
| @ComponentScan           | Spring组件扫描                                     |

## MYBATIS

mybatis执行流程

①读取MyBatis配置文件: mybatis-config.xml加载运行环境和映射文件
②构造会话工厂SqlSessionFactory
③会话工厂创建SqISession对象(包含了执行SQL语句的所有方法)
④操作数据库的接口，Executor执行器， 同时负责查询缓存的维护
⑤Executor接口的执行方法中有一个MappedStatement类型的参数, 封装了映射信息
⑥输入参数映射
⑦输出结果映射

mybatis支持延迟加载，如果存在一对多的关系,并且使用分步查询的时候就能使用延迟加载，开启方法在<association>标签中设置属性fetchType=lazy

延迟加载原理:

![image-20231118205701487](mdPic/面试题知识点/image-20231118205701487.png)

mybatis的一级和二级缓存

一级缓存：基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当Session进行flush或close之后，该Session中的所有Cache就将清空，默认打开一级缓存

二级缓存： 是基于namespace和mapper的作用域起作用的，不是依赖于SQL session，默认也是采用 PerpetualCache
HashMap 存储

1，对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了新增、修改、删除操作后，默认该作用域下所有 select 中的缓存将被clear
2，二级缓存需要缓存的数据实现Serializable接口
3，只有会话提交或者关闭以后，一级缓存中的数据才会转移到二级缓存中

脏数据：从目标中取出的数据已经过期、错误或者没有意义，这种数据就叫做脏数据

mybatis一级缓存的范围是一个事务之内，减少数据库的无效访问范围是sqlSession级别的 在有多个会话以及分布式场景会产生脏读现象
mybatis二级缓存是命名空间级别的，也就是Mapper.xml中的nameSpace属性增删改还会被失效，在分布式场景会被失效，实际用的也不多

# SpringCloud

组件有哪些：

注册中心 ：nacos euraka

配置中心：nacos

服务熔断降级：sentinel  hystrix

服务限流：sentinel

负载均衡：ribbon loadbalance

远程调用：openFeign

网关：gateway

**服务注册与发现是什么？**

服务注册是服务提供者与服务注册中心连接将自己的信息比如自身的服务名，ip端口等注册到服务注册中心上

服务发现   是服务提供者会定期去服务注册中心拉取指定服务的服务列表信息，获取指定服务的地址去访问该服务，如果服务存在集群则会使用负载均衡算法去选择一个合适的服务结点去访问

服务监控：  服务提供者会以固定的频率去访问注册中心，去告诉服务注册中心自己还存活者，如果服务注册中心很久没有收到一个服务的心跳，则会认为这个服务已经死了，将其移除服务列表

**nacos与Eureka的区别**

在nacos当中，可以将服务设置为，非临时实例，可以在配置中心当中配置ephemeral属性设置，默认是临时实例，

如果是临时实例的话，naocs会主动的去检测该实例的健康状况，并且该节点不存活之后，并不会在列表当中删除该节点的信息，而是会将该结点标记为不健康状态。

nacos会主动的向其他结点推送该结点变更的信息

Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式，Eureka采用AP方式

Nacos还支持配置中心，Eureka却不支持

**Spring cloud 服务的雪崩**

![image-20231118234646957](mdPic/面试题知识点/image-20231118234646957.png)

在微服务之间进行通信服务调用由于某一个服务故障，导致级联服务故障的现象叫做雪崩效应。雪崩效应描述的是提供方不可用，导致消费方不可用并将不可用逐渐放大的过程。

比如有三个服务：用户服务调用商品服务，商品服务调用库存服务，调用关系如下图：

![img](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0250cf1aeda547768243bbf7ee30f6f8~tplv-k3u1fbpfcp-zoom-in-crop-mark:1512:0:0:0.awebp)

如果有一时刻用户服务的流量波动很大，流量经常会突然性增加，那么在这种情况下，就算用户服务能够扛着请求，但是商品服务或者库存服务也未必能够扛住这突发性的海量请求。这时候库存服务很可能因为扛不住请求，而变得不可用，那么商品服务的请求也会变得阻塞，慢慢耗尽商品服务的线程资源，这时候商品服务也会变得不可用了，同理会导致用户服务也变得不可用。

所以服务雪崩产生的根本原因是在调用链路中链路的某一个服务因为执行业务时间过长，或者大规模出现异常导致自身服务不可用，并且将这种不可用进行放大的情况。就像雪崩一样，雪崩的出现往往是因为山顶的一刻雪球的滚动，从而导致不断的放大，从而导致的雪崩的出现。

如果系统没有设计处理好，服务雪崩是微服务中很容易出现的情况，从而会导致整个系统的不可用性，造成难以估量的损失，那么如何解决微服务中如何解决服务雪崩的问题呢？

主要的解决方案有**服务熔断**和**服务降级**！

链路追踪skywalking

![image-20231119000227614](mdPic/面试题知识点/image-20231119000227614.png)

限流方式：

Nginx 

漏桶算法限流,控制速率

![image-20231119132314960](mdPic/面试题知识点/image-20231119132314960.png)

控制并发数，限制每个IP的连接数以及并发数

![image-20231119132720308](mdPic/面试题知识点/image-20231119132720308.png)

spring cloud gateway  网管 配置令牌桶

![image-20231119132834445](mdPic/面试题知识点/image-20231119132834445.png)

区别: 漏桶算法是以固定的速率去控制请求的，但是令牌桶算法可能会产生波动



CAP与BASE理论

C 数据的一致性，也就是说各个节点的数据要保持一致

A 可用性，该分布式系统一直保持能够对外提供服务的状态

P paration tolerance 分区容忍性,就是当因为网络问题的时候，一个分布式系统产生了分区，那么每一个分区还是需要像之前一样能够正常的对外提供服务

BASE 理论

BA  base available  基本可用,就是在系统出现故障的时候，允许损失部分的可用性，但是保证系统的核心功能可用

S  soft state 软状态  在一定时间内，允许出现一定的中间状态,比如可以出现临时的数据不一至等待事务协调者去协调各个服务

E   最终一致性  无法保证强一致性，但是在软状态结束之后会达到最终的一致性

![image-20231119140728559](mdPic/面试题知识点/image-20231119140728559.png)

解决分布式事务的思想和模型:
**最终一致思想**:各分支事务分别执行并提交，如果有不一致的情况，再想办法恢复数据(AP）

**强一致思想:**各分支事务执行完业务不要提交，等待彼此结果。而后统一提交或回滚(CP)



**seata解决分布式事务**

seata中的几个角色:

- TC   transacation  coordinator  事务协调者,维护全局的事务的状态,协调事务的回滚或提交
- TM  transaction manager  事务控制者，负责全局事务的发起,提交或者回滚全局事务
- RM  resource manager  资源管理者，负责管理分支事务，与TC交谈以注册分支事务和报告分支事务的状态,并驱动分支事务的回滚或提交

seata分布式事务的三种模式

**XA模式:**

等待各个分支事务完成之后才会实现提交或者回滚,性能相对其他两个较差是CP保证的数据的强一致性

![image-20231119145429144](mdPic/面试题知识点/image-20231119145429144.png)

**AT模式(最常用的)：**

各个分支事务会各自提交，并且分支事务提交的时候会使用undo_log去记录提交的信息,当其他分支的事务需要回滚的时候，已经提交的事务会更具undo_log来进行回滚,性能较好时AP

![image-20231119145728472](mdPic/面试题知识点/image-20231119145728472.png)

**TCC模式：**

该模式在对资源进行操作之前会对资源进行操作预检，并冻结资源,如果资源能够实现的话就提交事务，否则就释放资源，是实现了AP 但是代码耦合度较高，实现更加的复杂,性能较好

![image-20231119150003515](mdPic/面试题知识点/image-20231119150003515.png)

**需要实现幂等性的场景**

用户重复点击

MQ消息重复发送

超时重试机制

**考虑幂等性的场景:**

新增操作，不能重复新增，

按增量更新，如不能重复扣款等

**解决方案**

**按钮只可操作一次**

一般是提交后把按钮置灰或loding状态,消除用户因为重复点击而产生的重复记录,比如添加操作,由于点击两次而产生两条记录

**使用唯一索引防止新增脏数据**

利用数据库唯一索引机制,当数据重复时,插入数据库会抛出异常,保证不会出现脏数据。

**Token + Redis**

**以订单为例:**

第一阶段:在进入到提交订单页面之前,需要订单系统根据用户信息向后端发起一个申请Token的请求,后端将Token保存到Redis缓存中,为第二阶段操作使用。

第二阶段: 订单系统拿着申请到的token发起提交订单请求,后端会检查Redis中是否存在该Token, 如果存在, 表示第一次发起订单提交请求,开始逻辑处理,处理完逻辑后删除Redis中的Token 当有重复请求的时候,检查缓存中Token是否存在。不存在表示非法请求。

**状态机**

针对更新操作,比如业务上需要修改订单状态,例如订单状态有待支付、支付中、支付成功、支付失败、订单超时关闭等,在设计的时候最好只支持状态的单向改变(不可逆),也就是在更新的时候where条件里可以加上status = {状态},多次调用的话实际上也只会执行一次。

# java集合

![image-20231119164512633](mdPic/面试题知识点/image-20231119164512633.png)

ArrayList

![image-20231119171417086](mdPic/面试题知识点/image-20231119171417086.png)

ArrayList底层的实现原理是什么
●ArrayList底层 是用动态的数组实现的
●ArrayList初始容量为0， 当第一-次添加数据的时候才会初始化容量为10
●ArrayList在进行扩 容的时候是原来容量的1.5倍,每次扩容都需要拷贝数组
●ArrayList在添加数据的时候
	◆确保数组已使用长度(size) 加1之后足够存下下一个数据
	◆计算数组的容量，如果当前数组已使用长度+ 1后的大于当前的数组长度,则调用grow方法扩容(原来的1 .5倍)
	◆确保新增的数据有地方存储之 后，则将新元素添加到位于size的位置上。
	◆返回添加成功布尔值。

List与数组之间的转换:

```JAVA
Arrays.asList();//传入的这个数组再被修改之后会对这个方法产生的List产生影响,并且该方法返回的ArrayList并不是我们常用的那个

list.toArray();
```

ArrayList与LinkedList他们之间的区别是什么：

1. ArrayList底层是使用的动态扩容的数组实现的而LinkedList底层是使用的双向链表来实现的
2. ArrayList通过索引查找的元素是o(1)而LinkedList查找的效率是O(n),通过元素的值与删除的效率都是O(n).,但是linkedList新增删除查找的效率也是O(1),因为这个特点LinkedList也是Deque双端队列的实现类之一
3. ArrayList使用的数组是使用连续的空间存储着的，而LinkedList却是使用的链表，双向链表需要存储数据与指针更占用内存
4. 他们都不是线程安全的

二叉搜索树时间复杂度 log n 最坏情况退化成链表 o 

红黑树：

![image-20231119180134945](mdPic/面试题知识点/image-20231119180134945.png)

红黑树的性质：

- 结点要么红色要么黑色
- 根结点是黑色
- 叶子结点都是黑色的根节点
- 红色结点的子节点都是黑色
- 任意节点到叶子结点的所有路径都包含相同数目的黑色结点

再添加或者删除结点的时候如果违反了这些性质，就会发生旋转保证这些性质

查找新增与删除的时间复杂度都是O(log n)

![image-20231119191659856](mdPic/面试题知识点/image-20231119191659856.png)

jdk1.8之后

![image-20231119191806296](mdPic/面试题知识点/image-20231119191806296.png)

![image-20231119193640932](mdPic/面试题知识点/image-20231119193640932.png)

HashMap是懒惰加载，在创建对象时并没有初始化数组

在无参的构造函数中，设置了默认的加载因子是0.75

map put方法：

![image-20231119194215317](mdPic/面试题知识点/image-20231119194215317.png)

# HashMap源码

### 1 put方法流程

```java
public V put(K key, V value) {
    return putVal(hash(key), key, value, false, true);
}

final V putVal(int hash, K key, V value, boolean onlyIfAbsent,
                   boolean evict) {
    Node<K,V>[] tab; Node<K,V> p; int n, i;
    //判断数组是否未初始化
    if ((tab = table) == null || (n = tab.length) == 0)
        //如果未初始化，调用resize方法 进行初始化
        n = (tab = resize()).length;
    //通过 & 运算求出该数据（key）的数组下标并判断该下标位置是否有数据
    if ((p = tab[i = (n - 1) & hash]) == null)
        //如果没有，直接将数据放在该下标位置
        tab[i] = newNode(hash, key, value, null);
    //该数组下标有数据的情况
    else {
        Node<K,V> e; K k;
        //判断该位置数据的key和新来的数据是否一样
        if (p.hash == hash &&
            ((k = p.key) == key || (key != null && key.equals(k))))
            //如果一样，证明为修改操作，该节点的数据赋值给e,后边会用到
            e = p;
        //判断是不是红黑树
        else if (p instanceof TreeNode)
            //如果是红黑树的话，进行红黑树的操作
            e = ((TreeNode<K,V>)p).putTreeVal(this, tab, hash, key, value);
        //新数据和当前数组既不相同，也不是红黑树节点，证明是链表
        else {
            //遍历链表
            for (int binCount = 0; ; ++binCount) {
                //判断next节点，如果为空的话，证明遍历到链表尾部了
                if ((e = p.next) == null) {
                    //把新值放入链表尾部
                    p.next = newNode(hash, key, value, null);
                    //因为新插入了一条数据，所以判断链表长度是不是大于等于8
                    if (binCount >= TREEIFY_THRESHOLD - 1) // -1 for 1st
                        //如果是，进行转换红黑树操作
                        treeifyBin(tab, hash);
                    break;
                }
                //判断链表当中有数据相同的值，如果一样，证明为修改操作
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    break;
                //把下一个节点赋值为当前节点
                p = e;
            }
        }
        //判断e是否为空（e值为修改操作存放原数据的变量）
        if (e != null) { // existing mapping for key
            //不为空的话证明是修改操作，取出老值
            V oldValue = e.value;
            //一定会执行  onlyIfAbsent传进来的是false
            if (!onlyIfAbsent || oldValue == null)
                //将新值赋值当前节点
                e.value = value;
            afterNodeAccess(e);
            //返回老值
            return oldValue;
        }
    }
    //计数器，计算当前节点的修改次数
    ++modCount;
    //当前数组中的数据数量如果大于扩容阈值
    if (++size > threshold)
        //进行扩容操作
        resize();
    //空方法
    afterNodeInsertion(evict);
    //添加操作时 返回空值
    return null;
}
```

### 2 扩容

```java
//扩容、初始化数组
final Node<K,V>[] resize() {
        Node<K,V>[] oldTab = table;
    	//如果当前数组为null的时候，把oldCap老数组容量设置为0
        int oldCap = (oldTab == null) ? 0 : oldTab.length;
        //老的扩容阈值
    	int oldThr = threshold;
        int newCap, newThr = 0;
        //判断数组容量是否大于0，大于0说明数组已经初始化
    	if (oldCap > 0) {
            //判断当前数组长度是否大于最大数组长度
            if (oldCap >= MAXIMUM_CAPACITY) {
                //如果是，将扩容阈值直接设置为int类型的最大数值并直接返回
                threshold = Integer.MAX_VALUE;
                return oldTab;
            }
            //如果在最大长度范围内，则需要扩容  OldCap << 1等价于oldCap*2
            //运算过后判断是不是最大值并且oldCap需要大于16
            else if ((newCap = oldCap << 1) < MAXIMUM_CAPACITY &&
                     oldCap >= DEFAULT_INITIAL_CAPACITY)
                newThr = oldThr << 1; // double threshold  等价于oldThr*2
        }
    	//如果oldCap<0，但是已经初始化了，像把元素删除完之后的情况，那么它的临界值肯定还存在，       			如果是首次初始化，它的临界值则为0
        else if (oldThr > 0) // initial capacity was placed in threshold
            newCap = oldThr;
        //数组未初始化的情况，将阈值和扩容因子都设置为默认值
    	else {               // zero initial threshold signifies using defaults
            newCap = DEFAULT_INITIAL_CAPACITY;
            newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);
        }
    	//初始化容量小于16的时候，扩容阈值是没有赋值的
        if (newThr == 0) {
            //创建阈值
            float ft = (float)newCap * loadFactor;
            //判断新容量和新阈值是否大于最大容量
            newThr = (newCap < MAXIMUM_CAPACITY && ft < (float)MAXIMUM_CAPACITY ?
                      (int)ft : Integer.MAX_VALUE);
        }
    	//计算出来的阈值赋值
        threshold = newThr;
        @SuppressWarnings({"rawtypes","unchecked"})
        //根据上边计算得出的容量 创建新的数组       
    	Node<K,V>[] newTab = (Node<K,V>[])new Node[newCap];
    	//赋值
    	table = newTab;
    	//扩容操作，判断不为空证明不是初始化数组
        if (oldTab != null) {
            //遍历数组
            for (int j = 0; j < oldCap; ++j) {
                Node<K,V> e;
                //判断当前下标为j的数组如果不为空的话赋值个e，进行下一步操作
                if ((e = oldTab[j]) != null) {
                    //将数组位置置空
                    oldTab[j] = null;
                    //判断是否有下个节点
                    if (e.next == null)
                        //如果没有，就重新计算在新数组中的下标并放进去
                        newTab[e.hash & (newCap - 1)] = e;
                   	//有下个节点的情况，并且判断是否已经树化
                    else if (e instanceof TreeNode)
                        //进行红黑树的操作
                        ((TreeNode<K,V>)e).split(this, newTab, j, oldCap);
                    //有下个节点的情况，并且没有树化（链表形式）
                    else {
                        //比如老数组容量是16，那下标就为0-15
                        //扩容操作*2，容量就变为32，下标为0-31
                        //低位：0-15，高位16-31
                        //定义了四个变量
                        //        低位头          低位尾
                        Node<K,V> loHead = null, loTail = null;
                        //        高位头		   高位尾
                        Node<K,V> hiHead = null, hiTail = null;
                        //下个节点
                        Node<K,V> next;
                        //循环遍历
                        do {
                            //取出next节点
                            next = e.next;
                            //通过 与操作 计算得出结果为0
                            if ((e.hash & oldCap) == 0) {
                                //如果低位尾为null，证明当前数组位置为空，没有任何数据
                                if (loTail == null)
                                    //将e值放入低位头
                                    loHead = e;
                                //低位尾不为null，证明已经有数据了
                                else
                                    //将数据放入next节点
                                    loTail.next = e;
                                //记录低位尾数据
                                loTail = e;
                            }
                            //通过 与操作 计算得出结果不为0
                            else {
                                 //如果高位尾为null，证明当前数组位置为空，没有任何数据
                                if (hiTail == null)
                                    //将e值放入高位头
                                    hiHead = e;
                                //高位尾不为null，证明已经有数据了
                                else
                                    //将数据放入next节点
                                    hiTail.next = e;
                               //记录高位尾数据
                               	hiTail = e;
                            }
                            
                        } 
                        //如果e不为空，证明没有到链表尾部，继续执行循环
                        while ((e = next) != null);
                        //低位尾如果记录的有数据，是链表
                        if (loTail != null) {
                            //将下一个元素置空
                            loTail.next = null;
                            //将低位头放入新数组的原下标位置
                            newTab[j] = loHead;
                        }
                        //高位尾如果记录的有数据，是链表
                        if (hiTail != null) {
                            //将下一个元素置空
                            hiTail.next = null;
                            //将高位头放入新数组的(原下标+原数组容量)位置
                            newTab[j + oldCap] = hiHead;
                        }
                    }
                }
            }
        }
    	//返回新的数组对象
        return newTab;
    }
```



### 3 get方法

```java
public V get(Object key) {
    Node<K,V> e;
    //hash(key)，获取key的hash值
    //调用getNode方法，见下面方法
    return (e = getNode(hash(key), key)) == null ? null : e.value;
}


final Node<K,V> getNode(int hash, Object key) {
    Node<K,V>[] tab; Node<K,V> first, e; int n; K k;
    //找到key对应的桶下标，赋值给first节点
    if ((tab = table) != null && (n = tab.length) > 0 &&
        (first = tab[(n - 1) & hash]) != null) {
        //判断hash值和key是否相等，如果是，则直接返回，桶中只有一个数据（大部分的情况）
        if (first.hash == hash && // always check first node
            ((k = first.key) == key || (key != null && key.equals(k))))
            return first;
        
        if ((e = first.next) != null) {
            //该节点是红黑树，则需要通过红黑树查找数据
            if (first instanceof TreeNode)
                return ((TreeNode<K,V>)first).getTreeNode(hash, key);
            
            //链表的情况，则需要遍历链表查找数据
            do {
                if (e.hash == hash &&
                    ((k = e.key) == key || (key != null && key.equals(k))))
                    return e;
            } while ((e = e.next) != null);
        }
    }
    return null;
}
```

put的流程

1.判断键值对数组table是否为空或为null,否则执行resize()进行扩 容(初始化)
2.根据键值key计算hash值得到数组索引|
3.判断table[i]==null,条件成立,直接新建节点添加
4.如果table[i]= =null ,不成立
	4.1判断table[i]的首个元素是否和key-样,如果相同直接覆盖value
	4.2判断table[i]是否为treeNode,即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对
	4.3遍历table[i],链表的尾部插入数据，然后判断链表长度是否大于8,大于8的话把链表转换为红黑树，在红黑树中执行插入
	操作,遍历过程中若发现key已经存在直接覆盖value
5.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold (数组长度*0.75) ，如果超过,进行扩容。

![image-20231119200811405](mdPic/面试题知识点/image-20231119200811405.png)

**hashMap的hash()方法：**

```
  static final int hash(Object key) {
        int h;
        return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
    }
```

高区的16位很有可能会被数组槽位数的二进制码锁屏蔽，如果我们不做刚才移位异或运算，那么在计算槽位时将丢失高区特征

也许你可能会说，即使丢失了高区特征不同hashcode也可以计算出不同的槽位来，但是细想当两个哈希码很接近时，那么这高区的一点点差异就可能导致一次哈希碰撞，所以这也是将性能做到极致的一种体现
异或运算能更好的保留各部分的特征，如果采用&运算计算出来的值会向0靠拢，采用|运算计算出来的值会向1靠拢

**计算数组下标**

使用位运算效率更高   

在计算散列位置时 `k = j - 1 & i` ，理论上是将hash值对散列表长度 `j` （默认长度 16）取模  k = i  %  j，实际则转换成了与运算,但是要满足条件，自定义 HashMap 容量应为 2 的幂次方

(n-1)&hash:得到数组中的索引，代替取模，性能更好数组长度必须是2的n次幂

![image-20231119205429733](mdPic/面试题知识点/image-20231119205429733.png)

![image-20231119205444321](mdPic/面试题知识点/image-20231119205444321.png)

hashMap在jdk1.7中链表是头插法，在高并发情况下可能会有死循环问题

# JUC

**线程与进程的区别**

1. 进程是正在运行的程序实例，而一个进程当中可以包括很多的线程
2. 进程与进程之间的内存不是共享的，但是一个进程中的所有线程共享该进程的数据
3. 线程更加的轻量，相较于进程，它的上下文切换成本更高

并发是交替做某几件事情

并行是同一时间同时的做几件事情

Thread的几个状态：

```java
public enum State {
    	//新建
        NEW,
    	//就绪，运行
        RUNNABLE,
    	//阻塞  没有拿到锁
        BLOCKED,
    	//  wait()
        WAITING,
    	// sleep()
        TIMED_WAITING,
    	// 运行完成
        TERMINATED;
    }
```

![image-20231119232357908](mdPic/面试题知识点/image-20231119232357908.png)





三个线程如何按顺序进行，可以使用 join()方法，他可以等待某一个方法执行结束以后在执行，也可以使用completableFurtrue类

notify()只会随机唤醒一个被wait的线程，但是notifyAll会唤醒所有